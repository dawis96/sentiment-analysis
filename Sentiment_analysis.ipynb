{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first think another Disney movie, might good, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Put aside Dr. House repeat missed, Desperate H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big fan Stephen King's work, film made even gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watched horrid thing TV. Needless say one movi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>truly enjoyed film. acting terrific plot. Jeff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       SentimentText  Sentiment\n",
       "0  first think another Disney movie, might good, ...          1\n",
       "1  Put aside Dr. House repeat missed, Desperate H...          0\n",
       "2  big fan Stephen King's work, film made even gr...          1\n",
       "3  watched horrid thing TV. Needless say one movi...          0\n",
       "4  truly enjoyed film. acting terrific plot. Jeff...          1"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sentiment_movies.csv', encoding = \"ISO-8859-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analiza zbioru danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'klasa')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = df['Sentiment'].value_counts().plot(kind='bar')\n",
    "ax.set_title(\"liczba wystąpień każdej z klas\")\n",
    "ax.set_ylabel(\"ilość\")\n",
    "ax.set_xlabel(\"klasa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alright, admit never seen \"Rhoda\" one two episodes \"The Mary Tyler Moore Show.\" Even though don\\'t know anything duo comedic talent, still liked movie lot.<br /><br />Mary goes back work. Rose tries luck comedian. Rhoda struggles photography career. Meredith...what exactly again? three stories follow two hours amusing entertaining way. two long time friends reunite, makes film better. <br /><br />I surprised good writing was. little jokes thrown Mary Rhoda funny. script well put together.<br /><br />I seen Moore Harper movies past years thought good. idea worked well team. actresses share fulfill title movie, never seem let down. (During run movie.) Joie Lenz Marisa Ryan play roles okay nothing great. rest cast like Jonah, Cecile and....everybody else also works well together.<br /><br />Being reunion, would expect fan either show enjoy this. non-fan still enjoyed little get-together. Good story lines character two main characters makes film good. (The newer version MTM theme song doesn\\'t hurt either.)'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wypisanie przykladowej recenzji\n",
    "df.loc[1234, 'SentimentText'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesowanie tekstu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Czyszczenie \n",
    "Jak widac powyzej dane posiadaja znaki specjalne html, ktore nalezy usunac. W celu wyciagniecia z danych tylko slow trzeba usunac rowniez znaki takie jak \".\" \"!\" \"?\" itp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zastapienie znakow spacjami:\n",
    "df['SentimentText_cleaned'] = df['SentimentText'].apply(lambda x: re.sub(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\", \" \", x))\n",
    "df['SentimentText_cleaned'] = df['SentimentText_cleaned'].apply(lambda x: x.replace(\"_\", \" \"))\n",
    "# Usuniecie znakow specjalnych:\n",
    "df['SentimentText_cleaned'] = df['SentimentText_cleaned'].apply(lambda x: re.sub(\"[\\W]+\", ' ', x))\n",
    "# Zamiana wszystkich liter na male:\n",
    "df['SentimentText_cleaned'] = df['SentimentText_cleaned'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usuniecie cyfr\n",
    "df['SentimentText_cleaned'] = df['SentimentText_cleaned'].apply(lambda x: re.sub(\"\\d+\", ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alright admit never seen rhoda one two episodes the mary tyler moore show even though don t know anything duo comedic talent still liked movie lot mary goes back work rose tries luck comedian rhoda struggles photography career meredith what exactly again three stories follow two hours amusing entertaining way two long time friends reunite makes film better i surprised good writing was little jokes thrown mary rhoda funny script well put together i seen moore harper movies past years thought good idea worked well team actresses share fulfill title movie never seem let down during run movie joie lenz marisa ryan play roles okay nothing great rest cast like jonah cecile and everybody else also works well together being reunion would expect fan either show enjoy this non fan still enjoyed little get together good story lines character two main characters makes film good the newer version mtm theme song doesn t hurt either '"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# przykladowa recencja po wstepnym oczyszczeniu\n",
    "df.loc[1234, 'SentimentText_cleaned'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usuniecie 'stop-words'\n",
    "Usuniecie bardzo popularnych slow takich jak \"i\", \"jesli\", \"ona\", które nic nie wnosza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'that', 'through', 'own', 'ma']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# przyklady angielskig 'stop-words':\n",
    "english_stop_words[::40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SentimentText_cleaned_no_stops'] = df['SentimentText_cleaned'].apply(lambda x:\n",
    "                                                                          ' '.join([word for word in x.split() \n",
    "                                                                                    if word not in english_stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alright admit never seen rhoda one two episodes mary tyler moore show even though know anything duo comedic talent still liked movie lot mary goes back work rose tries luck comedian rhoda struggles photography career meredith exactly three stories follow two hours amusing entertaining way two long time friends reunite makes film better surprised good writing little jokes thrown mary rhoda funny script well put together seen moore harper movies past years thought good idea worked well team actresses share fulfill title movie never seem let run movie joie lenz marisa ryan play roles okay nothing great rest cast like jonah cecile everybody else also works well together reunion would expect fan either show enjoy non fan still enjoyed little get together good story lines character two main characters makes film good newer version mtm theme song hurt either'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recenzja po usunieciu 'stop-words':\n",
    "df.loc[1234, 'SentimentText_cleaned_no_stops'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normalizacja (steemming) slow \n",
    "Steeming to proces usuniecia ze slowa koncowki fleksyjnej pozostawiajac tylko temat wyrazu.\n",
    "W celu uzyskania lepszych wynikow przetestuje 2 algorytmy: Porter oraz Snowball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "df['SentimentText_porter'] = df['SentimentText_cleaned_no_stops'].apply(lambda x: ' '.join([porter.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alright admit never seen rhoda one two episod mari tyler moor show even though know anyth duo comed talent still like movi lot mari goe back work rose tri luck comedian rhoda struggl photographi career meredith exactli three stori follow two hour amus entertain way two long time friend reunit make film better surpris good write littl joke thrown mari rhoda funni script well put togeth seen moor harper movi past year thought good idea work well team actress share fulfil titl movi never seem let run movi joie lenz marisa ryan play role okay noth great rest cast like jonah cecil everybodi els also work well togeth reunion would expect fan either show enjoy non fan still enjoy littl get togeth good stori line charact two main charact make film good newer version mtm theme song hurt either'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recenzja po normalizacji z uzyciem algorytmu Portera:\n",
    "df.loc[1234, 'SentimentText_porter'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer(language='english')\n",
    "df['SentimentText_snowball'] = df['SentimentText_cleaned_no_stops'].apply(lambda x: ' '.join([snowball.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alright admit never seen rhoda one two episod mari tyler moor show even though know anyth duo comed talent still like movi lot mari goe back work rose tri luck comedian rhoda struggl photographi career meredith exact three stori follow two hour amus entertain way two long time friend reunit make film better surpris good write littl joke thrown mari rhoda funni script well put togeth seen moor harper movi past year thought good idea work well team actress share fulfil titl movi never seem let run movi joie lenz marisa ryan play role okay noth great rest cast like jonah cecil everybodi els also work well togeth reunion would expect fan either show enjoy non fan still enjoy littl get togeth good stori line charact two main charact make film good newer version mtm theme song hurt either'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recenzja po normalizacji z uzyciem algorytmu snowball:\n",
    "df.loc[1234, 'SentimentText_snowball'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zamian recenzji na wektory oraz testowanie przeprocesowanych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SentimentText', 'Sentiment', 'SentimentText_cleaned',\n",
       "       'SentimentText_preprocessed_no_stops', 'SentimentText_cleaned_no_stops',\n",
       "       'SentimentText_porter', 'SentimentText_snowball'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SentimentText',\n",
       " 'SentimentText_cleaned',\n",
       " 'SentimentText_preprocessed_no_stops',\n",
       " 'SentimentText_cleaned_no_stops',\n",
       " 'SentimentText_porter',\n",
       " 'SentimentText_snowball']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all = list(df.drop(columns=['Sentiment']).columns)\n",
    "y = df.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SentimentText': 0.8808,\n",
       " 'SentimentText_cleaned': 0.88116,\n",
       " 'SentimentText_cleaned_no_stops': 0.8814,\n",
       " 'SentimentText_porter': 0.87836,\n",
       " 'SentimentText_snowball': 0.87836}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.mean(cv_dict['SentimentText'])\n",
    "{key: np.mean(value) for key, value in cv_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 5 artists>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "X = np.arange(5)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(X + 0.0, cv_dict['SentimentText'], color = 'b', width = 0.10)\n",
    "ax.bar(X + 0.10, cv_dict['SentimentText_cleaned'], color = 'g', width = 0.10)\n",
    "ax.bar(X + 0.20, cv_dict['SentimentText_cleaned_no_stops'], color = 'r', width = 0.10)\n",
    "ax.bar(X + 0.30, cv_dict['SentimentText_porter'], color = 'y', width = 0.10)\n",
    "ax.bar(X + 0.40, cv_dict['SentimentText_snowball'], color = 'c', width = 0.10)\n",
    "# ax.bar(X + 0.30, data[1], color = 'g', width = 0.25)\n",
    "# ax.bar(X + 0.50, data[2], color = 'r', width = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_dict = {}\n",
    "for x in X_all:\n",
    "    X = df[x]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    cv = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "    cv.fit(X_train)\n",
    "    X_train = cv.transform(X_train)\n",
    "    X_test = cv.transform(X_test)\n",
    "    acc = []\n",
    "    for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "        lr = LogisticRegression(C=c)\n",
    "        lr.fit(X_train, y_train)\n",
    "        accuracy = accuracy_score(y_test, lr.predict(X_test))\n",
    "#         print (\"Accuracy for C=%s: %s\" \n",
    "#                % (c, accuracy_score(y_test, lr.predict(X_test))))\n",
    "        acc.append(accuracy)\n",
    "    cv_dict[x] = acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "X_train = tfidf_vectorizer.transform(X_train)\n",
    "#X_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dict = {}\n",
    "for x in X_all:\n",
    "    X = df[x]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    cv = TfidfVectorizer()\n",
    "    cv.fit(X_train)\n",
    "    X_train = cv.transform(X_train)\n",
    "    X_test = cv.transform(X_test)\n",
    "    acc = []\n",
    "    for c in [0.01, 0.05, 0.25, 0.5, 1, 10, ]:\n",
    "    \n",
    "        lr = LogisticRegression(C=c)\n",
    "        lr.fit(X_train, y_train)\n",
    "        accuracy = accuracy_score(y_test, lr.predict(X_test))\n",
    "#         print (\"Accuracy for C=%s: %s\" \n",
    "#                % (c, accuracy_score(y_test, lr.predict(X_test))))\n",
    "        acc.append(accuracy)\n",
    "    tf_dict[x] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SentimentText': 0.85612,\n",
       " 'SentimentText_cleaned': 0.85808,\n",
       " 'SentimentText_preprocessed_no_stops': 0.8588000000000001,\n",
       " 'SentimentText_cleaned_no_stops': 0.8588000000000001,\n",
       " 'SentimentText_porter': 0.8588000000000001,\n",
       " 'SentimentText_snowball': 0.8598800000000001}"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{key: np.mean(value) for key, value in tf_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-bc03f63b2569>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m         \"\"\"\n\u001b[1;32m--> 997\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    998\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1031\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    941\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 943\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    944\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    327\u001b[0m                                                tokenize)\n\u001b[0;32m    328\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 329\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "cv.fit(X_train)\n",
    "X_train = cv.transform(X_train)\n",
    "X_test = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.06901009, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[100].toarray()[0][2000:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8718\n",
      "Accuracy for C=0.05: 0.8764\n",
      "Accuracy for C=0.25: 0.8758\n",
      "Accuracy for C=0.5: 0.8744\n",
      "Accuracy for C=1: 0.8698\n"
     ]
    }
   ],
   "source": [
    "# after processing\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8724\n",
      "Accuracy for C=0.05: 0.8772\n",
      "Accuracy for C=0.25: 0.8744\n",
      "Accuracy for C=0.5: 0.873\n",
      "Accuracy for C=1: 0.8708\n"
     ]
    }
   ],
   "source": [
    "# delete html signs and stopwords\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.873\n",
      "Accuracy for C=0.05: 0.8772\n",
      "Accuracy for C=0.25: 0.8736\n",
      "Accuracy for C=0.5: 0.8704\n",
      "Accuracy for C=1: 0.8678\n"
     ]
    }
   ],
   "source": [
    "# delete html signs and stopwords, lowercase and all special marks\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.873\n",
      "Accuracy for C=0.05: 0.8774\n",
      "Accuracy for C=0.25: 0.8736\n",
      "Accuracy for C=0.5: 0.8702\n",
      "Accuracy for C=1: 0.8678\n"
     ]
    }
   ],
   "source": [
    "# delete html signs and stopwords, lowercase and all special marks, delete _\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8732\n",
      "Accuracy for C=0.05: 0.8784\n",
      "Accuracy for C=0.25: 0.8756\n",
      "Accuracy for C=0.5: 0.8706\n",
      "Accuracy for C=1: 0.8682\n"
     ]
    }
   ],
   "source": [
    "# delete html signs and stopwords, lowercase and all special marks, delete _ and digits\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.867\n",
      "Accuracy for C=0.05: 0.8738\n",
      "Accuracy for C=0.25: 0.8724\n",
      "Accuracy for C=0.5: 0.871\n",
      "Accuracy for C=1: 0.8666\n"
     ]
    }
   ],
   "source": [
    "# delete html signs and stopwords, lowercase and all special marks, delete _ and digits, porter\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8728\n",
      "Accuracy for C=0.05: 0.879\n",
      "Accuracy for C=0.25: 0.881\n",
      "Accuracy for C=0.5: 0.8808\n",
      "Accuracy for C=1: 0.8812\n"
     ]
    }
   ],
   "source": [
    "# delete html signs and stopwords, lowercase and all special marks, delete _ and digits, porter, ngram = 3\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-543d65d3a51d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#X_test = tfidf_vectorizer.transform(X_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1590\u001b[0m         \"\"\"\n\u001b[0;32m   1591\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1592\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1593\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1594\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1031\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    941\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 943\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    944\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    327\u001b[0m                                                tokenize)\n\u001b[0;32m    328\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 329\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "X_train = tfidf_vectorizer.transform(X_train)\n",
    "#X_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8068\n",
      "Accuracy for C=0.05: 0.8442\n",
      "Accuracy for C=0.25: 0.8684\n",
      "Accuracy for C=0.5: 0.8784\n",
      "Accuracy for C=1: 0.8828\n",
      "Accuracy for C=2: 0.8846\n",
      "Accuracy for C=3: 0.886\n"
     ]
    }
   ],
   "source": [
    "# delete html signs and stopwords, lowercase and all special marks, delete _ and digits, porter, ngram = 3\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1, 2, 3]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8174\n",
      "Accuracy for C=0.05: 0.845\n",
      "Accuracy for C=0.25: 0.8692\n",
      "Accuracy for C=0.5: 0.8778\n",
      "Accuracy for C=1: 0.881\n",
      "Accuracy for C=2: 0.8848\n",
      "Accuracy for C=3: 0.8866\n"
     ]
    }
   ],
   "source": [
    "# delete html signs and stopwords, lowercase and all special marks, delete _ and digits, porter, ngram = 3\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1, 2, 3]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8186\n",
      "Accuracy for C=0.05: 0.8458\n",
      "Accuracy for C=0.25: 0.8702\n",
      "Accuracy for C=0.5: 0.8764\n",
      "Accuracy for C=1: 0.8842\n",
      "Accuracy for C=2: 0.8894\n",
      "Accuracy for C=3: 0.8882\n"
     ]
    }
   ],
   "source": [
    "# delete html signs and stopwords, lowercase and all special marks, delete _ and digits, porter, ngram = 3\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1, 2, 3]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
